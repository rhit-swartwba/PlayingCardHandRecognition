{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0990ec-e24a-4ecf-929d-40af9a80534c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\schrocjq\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "640 480\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "from BlackjackPlayer import BlackjackPlayer\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "cap_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "cap_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(cap_width, cap_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1dc1d7-47d8-4301-b2d0-2f4e2c9d9974",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 2H, 326.6ms\n",
      "Speed: 2.8ms preprocess, 326.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 KH, 355.5ms\n",
      "Speed: 2.6ms preprocess, 355.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 310.8ms\n",
      "Speed: 1.9ms preprocess, 310.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 264.5ms\n",
      "Speed: 1.8ms preprocess, 264.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 262.5ms\n",
      "Speed: 1.5ms preprocess, 262.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 271.5ms\n",
      "Speed: 2.9ms preprocess, 271.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 269.1ms\n",
      "Speed: 2.5ms preprocess, 269.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 259.8ms\n",
      "Speed: 1.8ms preprocess, 259.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 270.1ms\n",
      "Speed: 1.5ms preprocess, 270.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 273.9ms\n",
      "Speed: 1.5ms preprocess, 273.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 274.3ms\n",
      "Speed: 1.7ms preprocess, 274.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 268.9ms\n",
      "Speed: 1.9ms preprocess, 268.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 263.8ms\n",
      "Speed: 1.7ms preprocess, 263.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 269.7ms\n",
      "Speed: 1.6ms preprocess, 269.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 299.6ms\n",
      "Speed: 1.5ms preprocess, 299.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 284.0ms\n",
      "Speed: 2.2ms preprocess, 284.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 272.9ms\n",
      "Speed: 1.8ms preprocess, 272.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 254.3ms\n",
      "Speed: 1.5ms preprocess, 254.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 261.6ms\n",
      "Speed: 1.8ms preprocess, 261.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 1 AS, 261.4ms\n",
      "Speed: 2.4ms preprocess, 261.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 278.9ms\n",
      "Speed: 1.7ms preprocess, 278.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 289.6ms\n",
      "Speed: 2.3ms preprocess, 289.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 9H, 1 KH, 279.0ms\n",
      "Speed: 2.7ms preprocess, 279.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 285.4ms\n",
      "Speed: 3.2ms preprocess, 285.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 305.2ms\n",
      "Speed: 1.7ms preprocess, 305.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 9H, 1 KH, 1 QS, 274.4ms\n",
      "Speed: 1.8ms preprocess, 274.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 KS, 264.7ms\n",
      "Speed: 1.8ms preprocess, 264.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 261.3ms\n",
      "Speed: 1.6ms preprocess, 261.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 KS, 264.4ms\n",
      "Speed: 1.4ms preprocess, 264.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 QC, 1 10H, 1 KH, 262.9ms\n",
      "Speed: 1.6ms preprocess, 262.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 QC, 1 10H, 1 KH, 286.2ms\n",
      "Speed: 1.8ms preprocess, 286.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 QS, 262.9ms\n",
      "Speed: 1.6ms preprocess, 262.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 KH, 263.8ms\n",
      "Speed: 2.2ms preprocess, 263.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 QC, 1 9H, 1 KH, 269.1ms\n",
      "Speed: 1.5ms preprocess, 269.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 9H, 265.0ms\n",
      "Speed: 1.8ms preprocess, 265.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 265.2ms\n",
      "Speed: 1.6ms preprocess, 265.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 272.7ms\n",
      "Speed: 1.9ms preprocess, 272.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 10Hs, 260.0ms\n",
      "Speed: 2.5ms preprocess, 260.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 260.1ms\n",
      "Speed: 1.5ms preprocess, 260.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 266.2ms\n",
      "Speed: 1.5ms preprocess, 266.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 269.5ms\n",
      "Speed: 1.6ms preprocess, 269.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 258.2ms\n",
      "Speed: 1.6ms preprocess, 258.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 270.0ms\n",
      "Speed: 1.7ms preprocess, 270.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 276.5ms\n",
      "Speed: 3.4ms preprocess, 276.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 266.4ms\n",
      "Speed: 1.8ms preprocess, 266.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 277.1ms\n",
      "Speed: 1.8ms preprocess, 277.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 JH, 1 AS, 257.2ms\n",
      "Speed: 2.2ms preprocess, 257.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 255.7ms\n",
      "Speed: 1.5ms preprocess, 255.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 253.3ms\n",
      "Speed: 1.5ms preprocess, 253.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 260.9ms\n",
      "Speed: 1.6ms preprocess, 260.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 261.0ms\n",
      "Speed: 1.7ms preprocess, 261.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 263.2ms\n",
      "Speed: 1.5ms preprocess, 263.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 AS, 275.2ms\n",
      "Speed: 1.6ms preprocess, 275.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 262.3ms\n",
      "Speed: 1.7ms preprocess, 262.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 257.8ms\n",
      "Speed: 1.4ms preprocess, 257.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 265.2ms\n",
      "Speed: 1.7ms preprocess, 265.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 264.5ms\n",
      "Speed: 2.7ms preprocess, 264.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 8H, 1 10H, 265.2ms\n",
      "Speed: 1.8ms preprocess, 265.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 10H, 1 KH, 1 3S, 269.2ms\n",
      "Speed: 2.1ms preprocess, 269.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(annotated_frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayer Cards: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(p_cards_detected)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m20\u001b[39m, cap_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m25\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.4\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     81\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(annotated_frame, (cap_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cap_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m), (cap_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m cap_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m6\u001b[39m, cap_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m cap_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m), (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFILLED)\n\u001b[1;32m---> 82\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(annotated_frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlay: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcomputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (cap_width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cap_height \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m25\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.4\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     85\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetection\u001b[39m\u001b[38;5;124m\"\u001b[39m, annotated_frame)\n\u001b[0;32m     88\u001b[0m key_press \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mC:\\CSSE463\\PlayingCardHandRecognition\\BlackjackPlayer.py:118\u001b[0m, in \u001b[0;36mBlackjackPlayer.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLACKJACK!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhard_value_map[player_card_list[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhard_value_map[player_card_list[\u001b[38;5;241m1\u001b[39m]]:\n\u001b[1;32m--> 118\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhard_value_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer_card_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhard_value_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdealer_card_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(player_card_list) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Define the zone polygon\n",
    "ZONE_POLYGON = np.array([\n",
    "    [0,0],\n",
    "    [cap_width ,0],\n",
    "    [cap_width,cap_height // 2],\n",
    "    [0,cap_height // 2]\n",
    "])\n",
    "\n",
    "model = YOLO('./bestCardDetector.pt')\n",
    "\n",
    "bounding_box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "zone = sv.PolygonZone(polygon=ZONE_POLYGON)\n",
    "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color(255,0,0))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Image capture failed\")\n",
    "        break\n",
    "\n",
    "    result = model(frame, agnostic_nms=True)[0]\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "    # List to store detected cards in the zone\n",
    "    d_cards_detected = []\n",
    "    p_cards_detected = []\n",
    "\n",
    "    d_idx = []\n",
    "    p_idx = []\n",
    "\n",
    "    # Process detections\n",
    "    for i in range(len(detections)):\n",
    "        xyxy_tensor = detections[i].xyxy\n",
    "        xyxy = xyxy_tensor.squeeze()\n",
    "        xmin, ymin, xmax, ymax = xyxy.astype(int)\n",
    "\n",
    "        classidx = int(detections[i].class_id)\n",
    "        classname = model.model.names[classidx]\n",
    "        conf = detections[i].confidence.item()\n",
    "\n",
    "        if zone.trigger(detections[i])[0]:\n",
    "            d_cards_detected.append(classname)\n",
    "            d_idx.append(classidx)\n",
    "        else :\n",
    "            p_cards_detected.append(classname)\n",
    "            p_idx.append(classidx)\n",
    "\n",
    "    computer = BlackjackPlayer(d_idx, p_idx, model.model.names)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    if len(detections) > 0:\n",
    "        labels = [\n",
    "            f\"{model.model.names[class_id]} {confidence:.2f}\"\n",
    "            for class_id, confidence in zip(detections.class_id, detections.confidence)\n",
    "        ]\n",
    "    else:\n",
    "        labels = []\n",
    "\n",
    "    annotated_frame = bounding_box_annotator.annotate(\n",
    "        scene=frame, detections=detections\n",
    "    )\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame, detections=detections, labels=labels\n",
    "    )\n",
    "\n",
    "\n",
    "    # annotated_frame = zone_annotator.annotate(scene=annotated_frame, label=None)\n",
    "    cv2.rectangle(annotated_frame, (0, 0), (cap_width, cap_height // 2), (0, 0, 255), thickness=2)\n",
    "\n",
    "     # Display detected cards in the zones\n",
    "    cv2.rectangle(annotated_frame, (10, 10), (cap_width  // 3, cap_height // 16), (50, 50, 50), cv2.FILLED)\n",
    "    cv2.putText(annotated_frame, f'Dealer Cards: {\", \".join(d_cards_detected)}', (20, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "    cv2.rectangle(annotated_frame, (10, cap_height // 2 + 10), (cap_width // 3, cap_height // 2 + cap_height // 16), (50, 50, 50), cv2.FILLED)\n",
    "    cv2.putText(annotated_frame, f'Player Cards: {\", \".join(p_cards_detected)}', (20, cap_height // 2 + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "    cv2.rectangle(annotated_frame, (cap_width // 2, cap_height // 2 + 10), (cap_width // 2 + cap_width // 6, cap_height // 2 + cap_height // 16), (50, 50, 50), cv2.FILLED)\n",
    "    cv2.putText(annotated_frame, f'Play: {computer.play()}', (cap_width // 2, cap_height // 2 + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"detection\", annotated_frame)\n",
    "\n",
    "\n",
    "    key_press = cv2.waitKey(5)\n",
    "    if key_press == ord('q'):\n",
    "        break\n",
    "    elif key_press == ord('p'):\n",
    "        cv2.imwrite('table.png', annotated_frame)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f0235-8146-4db9-b4df-fa523ce6fcd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4a9bd-6d0f-483c-b90e-3e9a96fee1d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}